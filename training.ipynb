{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from res_att_unet import ResAttUnet\n",
    "from eval_metrics import calc_scores\n",
    "from utils import plot_graph\n",
    "\n",
    "from datetime import datetime\n",
    "from dataset import Datasets_1Step, Datasets_2Step, Datasets_3Step\n",
    "\n",
    "from res_att_unet import ResAttEncoder, ResAttUnet, MLP\n",
    "from nt_xent import NTXentLoss\n",
    "from supcon_loss import BlockConLoss\n",
    "from utils import EarlyStopping, format_time, plot_graph, plot_loss\n",
    "from validation import validate_model\n",
    "from dice_loss import SoftDiceLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters and Data Loading paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_LEARNING_RATE = 5e-4\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PRE_BATCH_SIZE = 2\n",
    "BATCH_SIZE = 4\n",
    "PRE_NUM_EPOCHS = 100\n",
    "NUM_EPOCHS = 150\n",
    "SEED = 42\n",
    "\n",
    "# Data paths\n",
    "# For pre-training in the first step\n",
    "PRE_TRAIN_IMG_DIR = \"./data/train_imgs.npz\"       # 100% of training dataset for pre-training\n",
    "\n",
    "# For training in the second step\n",
    "LABELED_IMG_DIR = \"./data/imgs/x_train_1.npz\"    # 10 of training images to treat as labeled\n",
    "LABELED_GTS_DIR = \"./data/gts/y_train_1.npz\"   # 10 of training images to treat as labeled\n",
    "UNLABELED_IMG_DIR = \"./data/unlabeled_imgs/x_ul_1.npz\"       # 90 of training images to treat as unlabeled\n",
    "PSEUDO_GTS_DIR = \"./data/pseudo_gts/pseudo_label_1.npz\"  # 90 of training images to treat as pseudo labels\n",
    "\n",
    "TRAIN_IMG_DIR = \"./data/train_imgs.npz\"  # 80% of whole dataset for training\n",
    "TRAIN_GTS_DIR = \"./data/train_gts.npz\"   # 80% of whole dataset for training\n",
    "VAL_IMG_DIR = \"./data/val_imgs.npz\"    # 20% of whole dataset for validation\n",
    "VAL_GTS_DIR = \"./data/val_gts.npz\"   # 20% of whole dataset for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preproprecessing for 1Step pre-training\n",
    "train_imgs_1step = np.load(PRE_TRAIN_IMG_DIR, allow_pickle=True)['data']\n",
    "train_data_1step = Datasets_1Step(train_imgs_1step)\n",
    "print(train_imgs_1step.shape)\n",
    "\n",
    "# Data preprocessing for 2Step pre-training\n",
    "labeled_imgs = np.load(LABELED_IMG_DIR, allow_pickle=True)['data']\n",
    "labeled_gts = np.load(LABELED_GTS_DIR, allow_pickle=True)['data']\n",
    "unlabeled_imgs = np.load(UNLABELED_IMG_DIR, allow_pickle=True)['data']\n",
    "pseudo_gts = np.load(PSEUDO_GTS_DIR, allow_pickle=True)['data']\n",
    "train_imgs_2step = np.concatenate([labeled_imgs, unlabeled_imgs], axis=0)\n",
    "train_gts_2step = np.concatenate([labeled_gts, pseudo_gts], axis=0)\n",
    "train_data_2step = Datasets_2Step(train_imgs_2step, train_gts_2step, transform=True)\n",
    "print(labeled_imgs.shape, labeled_gts.shape, unlabeled_imgs.shape, pseudo_gts.shape)\n",
    "\n",
    "# Data preprocessing for 3Step pre-training (segmentation training)\n",
    "train_dataset_3step = Datasets_3Step(labeled_imgs, labeled_gts, transform=True)\n",
    "\n",
    "\n",
    "# Data preprocessing for 3Step training (segmentation training)\n",
    "train_imgs = np.load(TRAIN_IMG_DIR, allow_pickle=True)['data']\n",
    "train_gts = np.load(TRAIN_GTS_DIR, allow_pickle=True)['data']\n",
    "val_imgs = np.load(VAL_IMG_DIR, allow_pickle=True)['data']\n",
    "val_gts = np.load(VAL_GTS_DIR, allow_pickle=True)['data']\n",
    "train_dataset = Datasets_3Step(train_imgs, train_gts, transform=True)\n",
    "val_dataset = Datasets_3Step(val_imgs, val_gts, transform=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRA FUNCTION FOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra functions for visualization\n",
    "selected_class_indice = [0, 1, 2, 3, 4]\n",
    "selected_class_rgb = [[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0]]\n",
    "\n",
    "def color_code_segment(image):\n",
    "    color_code = np.array(selected_class_rgb)\n",
    "    x = color_code[image.astype(int)]\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_overlay(image, mask, alpha=0.5):\n",
    "    # Ensure the image and mask are in the same shape\n",
    "    if mask.ndim == 2:  # If mask is grayscale, convert to RGB\n",
    "        mask = np.stack([mask] * 3, axis=-1)\n",
    "\n",
    "    if image.ndim == 2:\n",
    "        image = np.stack([image] * 3, axis=-1)\n",
    "\n",
    "    # Blend the images\n",
    "    overlay = alpha * mask + (1 - alpha) * image\n",
    "    return np.clip(overlay, 0, 1)  # Ensure values stay in [0, 1]\n",
    "\n",
    "import random\n",
    "\n",
    "def visualize(images, masks):\n",
    "    fontsize = 18\n",
    "    num_images = min(5, len(images))  # Limit to 10 images if there are more\n",
    "\n",
    "    # Randomly select indices from the whole dataset\n",
    "    indices = random.sample(range(len(images)), num_images)\n",
    "\n",
    "    f, ax = plt.subplots(3, num_images, figsize=(num_images * 4, 10))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        idx = indices[i]\n",
    "        image = images[idx] / 255.0 \n",
    "        ax[0, i].imshow(image, cmap='gray')\n",
    "        ax[0, i].set_title(f'Image Index: {idx}', fontsize=fontsize)\n",
    "        ax[0, i].axis('off')\n",
    "\n",
    "        ax[1, i].imshow(color_code_segment(masks[idx]))\n",
    "        ax[1, i].set_title(f'Ground Truth Index: {idx}', fontsize=fontsize)\n",
    "        ax[1, i].axis('off')\n",
    "\n",
    "        overlay = create_overlay(image, color_code_segment(masks[idx]))\n",
    "        ax[2, i].imshow(overlay)\n",
    "        ax[2, i].set_title(f\"Overlay Index: {idx}\", fontsize=fontsize)\n",
    "        ax[2, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIRST STEP PRE_TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for 1Step pre-training (encoder pre-training)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_classes=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.backbone = ResAttEncoder(in_channels=3)\n",
    "        self.projection_head = MLP(512, num_class=num_classes)\n",
    "        self.reconstruction_head = nn.ConvTranspose2d(in_channels=512, out_channels=3, kernel_size=8, stride=8)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z_1, _ = self.backbone(x1)\n",
    "        z_2, _ = self.backbone(x2)\n",
    "        z_recon_1 = self.reconstruction_head(z_1)\n",
    "        #z_recon_2 = self.reconstruction_head(z_2)\n",
    "\n",
    "        z_1 = self.projection_head(z_1)\n",
    "        z_2 = self.projection_head(z_2)\n",
    "\n",
    "        return x1, z_recon_1, z_1, z_2\n",
    "\n",
    "# Checkpoint paths\n",
    "NUM_EPOCHS_1Step = 200\n",
    "current_date = str(datetime.now().strftime(\"%Y_%m_%d\"))\n",
    "checkpoint_name = 'checkpoints/1step_pretraining_checkpoint.pth.tar'\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_data_1step, batch_size=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# Model for training\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "model = Encoder(num_classes=128).to(DEVICE)\n",
    "loss_nt = NTXentLoss(DEVICE, temperature=0.1, use_cosine_similarity=True)\n",
    "loss_mse = nn.MSELoss()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=PRE_LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "logging.disable(logging.NOTSET)\n",
    "logging.basicConfig(filename=os.path.join('log_file', 'pre_training.log'), level=logging.DEBUG)\n",
    "early_stopping = EarlyStopping(patience=25, verbose=True, indicator='loss')\n",
    "\n",
    "loss_trend = []\n",
    "loss_1_trend = []\n",
    "loss_2_trend = []\n",
    "beta = 0.1\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_1Step):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_1 = 0\n",
    "    epoch_loss_2 = 0\n",
    "    training_loss = 0\n",
    "    running_loss_1 = 0\n",
    "    running_loss_2 = 0\n",
    "    loop = tqdm(train_loader)\n",
    "    for i, (x1, x2) in enumerate(loop):\n",
    "        x1, x2 = x1.float().to(DEVICE), x2.float().to(DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            z_real, z_recon, z_anchors, z_positive = model(x1, x2)\n",
    "            loss_1 = loss_nt(z_anchors, z_positive)\n",
    "            loss_2 = loss_mse(z_real, z_recon)\n",
    "            loss = loss_1*beta + loss_2*(1 - beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        training_loss += loss.item()\n",
    "        epoch_loss = training_loss / len(train_loader)\n",
    "        running_loss_1 += loss_1.item()\n",
    "        epoch_loss_1 = running_loss_1 / len(train_loader)\n",
    "        running_loss_2 += loss_2.item()\n",
    "        epoch_loss_2 = running_loss_2 / len(train_loader)\n",
    "\n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"Loss is NaN. Stopping training.\")\n",
    "        break\n",
    "\n",
    "    loss_trend.append(epoch_loss)\n",
    "    loss_1_trend.append(epoch_loss_1)\n",
    "    loss_2_trend.append(epoch_loss_2)\n",
    "\n",
    "    logging.debug(f\"Epoch: {epoch + 1}\\tLoss: {epoch_loss :.8f}\")\n",
    "    print(f\"Epoch: {epoch + 1}\\tLOSS: {epoch_loss :.4f}\\tLOSS_1: {epoch_loss_1 :.4f}\\tLOSS_2: {epoch_loss_2 :.4f}\")\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'loss': epoch_loss,\n",
    "        'model_state_dict': model.backbone.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    early_stopping(epoch_loss, model.backbone, checkpoint, checkpoint_name)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopped\")\n",
    "        logging.info(\"Training has finished\")\n",
    "        logging.disable(logging.CRITICAL)\n",
    "        plot_graph(loss_trend, loss_1_trend, loss_2_trend)\n",
    "        break\n",
    "\n",
    "    if epoch == (NUM_EPOCHS_1Step - 1):\n",
    "        logging.info(\"Training has finished\")\n",
    "        logging.disable(logging.CRITICAL)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SECOND STEP PRE_TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Model for 2Step Pre-training (model pre-training)\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, num_classes=128):\n",
    "        super(UNET, self).__init__()\n",
    "        self.backbone = ResAttUnet(in_channels=3)\n",
    "        self.prejector = nn.Sequential(nn.Conv2d(64, 256, kernel_size=1),\n",
    "                                             nn.Conv2d(256, num_classes, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x)\n",
    "        y = self.prejector(y)\n",
    "        return y\n",
    "\n",
    "# Checkpoint paths\n",
    "current_date = str(datetime.now().strftime(\"%Y_%m_%d\"))\n",
    "checkpoint_name = 'checkpoints/2step_pretraining_checkpoint.pth.tar'\n",
    "\n",
    "# Dataloader\n",
    "dataloader = DataLoader(train_data_2step, batch_size=PRE_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model for training\n",
    "model = UNET(num_classes=128).to(DEVICE)\n",
    "loss_fn = BlockConLoss(temperature=0.1, block_size=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=PRE_LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "# Load Checkpoint\n",
    "checkpoint = 'checkpoints/1step_pretraining_checkpoint.pth.tar'\n",
    "stats = torch.load(checkpoint)\n",
    "model_state = stats['model_state_dict']\n",
    "model.backbone.encoder.load_state_dict(model_state)\n",
    "\n",
    "logging.disable(logging.NOTSET)\n",
    "logging.basicConfig(filename=os.path.join('log_file', 'pre_training_2step.log'), level=logging.DEBUG)\n",
    "logging.info(f\"\\n\\nStart SupCon training for {PRE_NUM_EPOCHS} epochs {current_date}\")\n",
    "early_stopping = EarlyStopping(patience=100, verbose=True, indicator='loss')\n",
    "\n",
    "losses = []\n",
    "for epoch in range(PRE_NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()  # Start time for the epoch\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    print(f\"Epoch: {(epoch + 1)} / {PRE_NUM_EPOCHS}\")\n",
    "    loop = tqdm(dataloader)\n",
    "    for i, (img_1, img_2, gt_1, gt_2) in enumerate(loop):\n",
    "        img_1, img_2 = img_1.float(), img_2.float()\n",
    "        gt_1, gt_2 = gt_1.long(), gt_2.long()\n",
    "\n",
    "        imgs = torch.cat([img_1, img_2], dim=0)\n",
    "        labels = torch.cat([gt_1, gt_2], dim=0).squeeze(1)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            imgs = imgs.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        bsz = imgs.shape[0] // 2\n",
    "        features = model(imgs)\n",
    "        features = F.normalize(features, p=2, dim=1)\n",
    "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
    "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
    "        l1, l2 = torch.split(labels, [bsz, bsz], dim=0)\n",
    "        labels = torch.cat([l1.unsqueeze(1), l2.unsqueeze(1)], dim=1)\n",
    "        loss = loss_fn(features, labels)\n",
    "\n",
    "        if loss.mean() == 0:\n",
    "            continue\n",
    "        mask = (loss != 0)\n",
    "        mask = mask.int().cuda()\n",
    "        loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        training_loss += loss.item()\n",
    "\n",
    "    epoch_loss = training_loss / len(dataloader)\n",
    "\n",
    "    # End time for the epoch\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time  # Time taken for the epoch\n",
    "    epoch_duration_formatted = format_time(epoch_duration)  # Format the epoch duration into HH:MM:SS\n",
    "\n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"Loss in NaN, training stoped\")\n",
    "        break\n",
    "\n",
    "    losses.append(epoch_loss)\n",
    "    logging.debug(f\"Epoch: {epoch + 1}\\t\\tLoss: {epoch_loss}\")\n",
    "    print(f\"Epoch: {epoch + 1}\\tLoss: {epoch_loss:.4f}\\tEpoch time: {epoch_duration_formatted}\")\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'loss': epoch_loss,\n",
    "        'model_state_dict': model.backbone.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    early_stopping(epoch_loss, model, checkpoint, checkpoint_name)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopped\")\n",
    "        logging.disable(logging.CRITICAL)\n",
    "        break\n",
    "\n",
    "    if epoch == PRE_NUM_EPOCHS - 1:\n",
    "        #torch.save(checkpoint, checkpoint_name)\n",
    "        logging.disable(logging.CRITICAL)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIRD STEP PRE_TRAINING (SEGMENTATION TRAININ)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for downstream training\n",
    "class FULL_UNET(nn.Module):\n",
    "    def __init__(self, num_class=5):\n",
    "        super(FULL_UNET, self).__init__()\n",
    "        self.backbone = ResAttUnet(in_channels=3)\n",
    "        self.projection_head = nn.Conv2d(64, num_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x)\n",
    "        output = self.projection_head(y)\n",
    "\n",
    "        return output\n",
    "    \n",
    "# Checkpoint paths\n",
    "current_date = str(datetime.now().strftime(\"%Y_%m_%d\"))\n",
    "checkpoint_name = 'checkpoints/3step_training_checkpoint_.pth.tar'\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset_3step, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# Model for training\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "model = FULL_UNET(num_class=5).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "dc_loss = SoftDiceLoss(batch_dice=True, do_bg=False, smooth=1.0, apply_nonlin=torch.nn.Softmax(dim=1))\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Load Checkpoint\n",
    "checkpoint = 'checkpoints/2step_pretraining_checkpoint.pth.tar'\n",
    "stats = torch.load(checkpoint)\n",
    "model_state = stats['model_state_dict']\n",
    "model.backbone.load_state_dict(model_state)\n",
    "\n",
    "logging.disable(logging.NOTSET)\n",
    "logging.basicConfig(filename=os.path.join('log_file', 'training.log'), level=logging.DEBUG)\n",
    "early_stopping = EarlyStopping(patience=30, verbose=True, indicator='dice')\n",
    "\n",
    "loss_trend = []\n",
    "val_loss_trend = []\n",
    "accuracy_trend = []\n",
    "dice_trend = []\n",
    "iou_trend = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    loop = tqdm(train_loader)\n",
    "    print(f\"============================ Epoch: {epoch + 1}/{NUM_EPOCHS} ============================\")\n",
    "    for idx, (x, y) in enumerate(loop):\n",
    "        x = x.float().to(DEVICE)\n",
    "        y = y.long().to(DEVICE)\n",
    "        y = y.squeeze(1)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            pred = model(x)\n",
    "            loss = ce_loss(pred, y) + dc_loss(pred, y)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        training_loss += loss.item()\n",
    "    epoch_loss = training_loss / len(train_loader)\n",
    "    if torch.isnan(loss).any():\n",
    "        print(\"Loss is nan, training stopped\")\n",
    "        break\n",
    "\n",
    "        # checkpoints\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'loss': loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    # check accuracy\n",
    "    stats = validate_model(model, val_loader, DEVICE, 5)\n",
    "    if stats is None:\n",
    "        print(\"Validation failed, stats is None\")\n",
    "        break\n",
    "    acc = stats.get('acc', 0)\n",
    "    iou = stats.get('iou', 0)\n",
    "    dice = stats.get('dice', 0)\n",
    "    val_loss = stats.get('val_loss', 0)\n",
    "    val_loss_trend.append(val_loss)\n",
    "\n",
    "    accuracy_trend.append(acc)\n",
    "    iou_trend.append(iou)\n",
    "    dice_trend.append(dice)\n",
    "    loss_trend.append(epoch_loss)\n",
    "    logging.debug(f\"Epoch: {epoch + 1}\\tLoss: {epoch_loss}\\tIoU: {iou}\\tDice: {dice}\")\n",
    "    print(f\"train_loss: {epoch_loss:.4f}\\tVal_loss: {val_loss:.4f}\\tIoU Score: {iou:.2f}\\tDice Score: {dice:.2f}\")\n",
    "\n",
    "    early_stopping(dice, model, checkpoint, checkpoint_name)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        logging.disable(logging.CRITICAL)\n",
    "        plot_graph(iou_trend, dice_trend)\n",
    "        plot_loss(loss_trend, val_loss_trend)\n",
    "        print(\"Early stopped\")\n",
    "        break\n",
    "\n",
    "    if epoch == NUM_EPOCHS - 1:\n",
    "        logging.disable(logging.CRITICAL)\n",
    "        plot_graph(iou_trend, dice_trend)\n",
    "        plot_loss(loss_trend, val_loss_trend)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VALIDATE AND TEST THE TRAINED MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice_score(output, target, class_id):\n",
    "    pred_mask = F.softmax(output, dim=1)\n",
    "    pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "    pred_mask = pred_mask == class_id\n",
    "    true_mask = target == class_id\n",
    "\n",
    "    confusion_vector = pred_mask / true_mask\n",
    "    tp = torch.sum(confusion_vector == 1).item()\n",
    "    fp = torch.sum(confusion_vector == float('inf')).item()\n",
    "    tn = torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    fn = torch.sum(confusion_vector == 0).item()\n",
    "\n",
    "    # Calculate Dice Score\n",
    "    dice_score = (2 * tp) / (2 * tp + fp + fn + 1e-4)\n",
    "    # Calculate IoU\n",
    "    iou = tp / (tp + fp + fn + 1e-4)\n",
    "    # Calculate Acc\n",
    "    acc = (tn + tp) / (tn + fp + fn + tp)\n",
    "\n",
    "    return dice_score, iou, acc\n",
    "\n",
    "def validate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    total_dice_scores = {class_id: 0.0 for class_id in range(num_classes)}\n",
    "    total_iou_scores = {class_id: 0.0 for class_id in range(num_classes)}\n",
    "    total_acc_scores = {class_id: 0.0 for class_id in range(num_classes)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader):\n",
    "            inputs = inputs.float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            for class_id in range(1, num_classes):\n",
    "                dice_score, iou, acc = calculate_dice_score(outputs, targets, class_id)\n",
    "                total_dice_scores[class_id] += dice_score\n",
    "                total_iou_scores[class_id] += iou\n",
    "                total_acc_scores[class_id] += acc\n",
    "\n",
    "    average_dice_scores = {class_id: total_dice_scores[class_id] / len(dataloader) * 100 for class_id in range(num_classes)}\n",
    "    average_iou_scores = {class_id: total_iou_scores[class_id] / len(dataloader) * 100 for class_id in range(num_classes)}\n",
    "    average_acc_scores = {class_id: total_acc_scores[class_id] / len(dataloader) * 100 for class_id in range(num_classes)}\n",
    "\n",
    "    mean_average_dice = sum(average_dice_scores.values()) / (len(average_dice_scores) - 1) \n",
    "    mean_average_iou = sum(average_iou_scores.values()) / (len(average_iou_scores) - 1) \n",
    "    mean_average_acc = sum(average_acc_scores.values()) / (len(average_acc_scores) - 1) \n",
    "\n",
    "    print(f\"Acc: {mean_average_acc:.2f}\\tIoU: {mean_average_iou:.2f}\\tDice: {mean_average_dice:.2f}\")\n",
    "\n",
    "    for class_id in range(1, num_classes):\n",
    "        print(f\"Acc {class_id}: {average_acc_scores[class_id]:.2f}\\tIoU {class_id}: {average_iou_scores[class_id]:.2f}\\tDice {class_id}: {average_dice_scores[class_id]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation on data\n",
    "trained_model = model.to(DEVICE)\n",
    "# Load Checkpoint\n",
    "checkpoint = 'checkpoints/3step_training_checkpoint.pth.tar'\n",
    "stats = torch.load(checkpoint)\n",
    "model_state = stats['model_state_dict']\n",
    "model.load_state_dict(model_state)\n",
    "\n",
    "validate_model(trained_model, val_loader, DEVICE, num_classes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOTTING SOME EXAMPLE FOR VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 158 #L3\n",
    "trained_model = model.to(DEVICE)\n",
    "\n",
    "# Convert test images to torch tensor\n",
    "image_tensor = torch.from_numpy(val_imgs)\n",
    "\n",
    "# Select a specific image and move it to GPU\n",
    "image = val_imgs[idx] / 255.0\n",
    "image = A.Resize(256, 256, interpolation=cv2.INTER_LINEAR)(image=image.transpose(1, 2, 0))['image']\n",
    "image =  torch.tensor(image, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2) # Add batch dimension\n",
    "#print(image.shape)\n",
    "image_gpu = image.to(DEVICE)\n",
    "\n",
    "# Convert test labels to torch tensor and select the same label\n",
    "label = val_gts[idx] \n",
    "label_cpu = A.Resize(256, 256, interpolation=cv2.INTER_NEAREST)(image=label)['image']  # Convert to HWC for Albumentations, resize, then back to numpy\n",
    "\n",
    "# Model prediction\n",
    "preds = trained_model(image_gpu)\n",
    "preds = F.softmax(preds, dim=1)\n",
    "pred_labels = torch.argmax(preds, dim=1)\n",
    "print(pred_labels.shape)\n",
    "\n",
    "# Move image, prediction, and label back to CPU for visualization\n",
    "image_cpu = image_gpu.squeeze().to('cpu').numpy()\n",
    "pred_cpu = pred_labels.squeeze().to('cpu').numpy()  \n",
    "\n",
    "# Print the shapes\n",
    "print(f\"image shape: {image_cpu.shape}\")\n",
    "print(f\"pred shape: {pred_cpu.shape}\")\n",
    "print(f\"label shape: {label_cpu.shape}\")\n",
    "\n",
    "class_names = ['MUS', 'IMAT', 'SAT', 'VAT']  \n",
    "class_ids = [1, 2, 3, 4] \n",
    "num_classes = len(class_ids)\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(2, num_classes+2, figsize=(5*(num_classes+2), 11), dpi=300)\n",
    "title_font = 20\n",
    "\n",
    "# Column 0: Input Image\n",
    "axs[0, 0].imshow(image_cpu.transpose(1, 2, 0), cmap='gray')\n",
    "axs[0, 0].set_title('Input', fontsize=title_font)\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "axs[1, 0].imshow(image_cpu.transpose(1, 2, 0), cmap='gray')\n",
    "axs[1, 0].set_title('Input', fontsize=title_font)\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "# Column 1: All Classes\n",
    "axs[0, 1].imshow(color_code_segment(label_cpu))\n",
    "axs[0, 1].set_title('Ground Truth', fontsize=title_font)\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "axs[1, 1].imshow(color_code_segment(pred_cpu))\n",
    "axs[1, 1].set_title('Predicted', fontsize=title_font)\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "# Function to plot masks for each class\n",
    "for i, (class_id, class_name) in enumerate(zip(class_ids, class_names)):\n",
    "    label_mask = (label_cpu == class_id)\n",
    "    pred_mask = (pred_cpu == class_id)\n",
    "\n",
    "    if not label_mask.any() and not pred_mask.any():\n",
    "        continue\n",
    "\n",
    "    label_class = label_cpu * label_mask\n",
    "    pred_class = pred_cpu * pred_mask\n",
    "\n",
    "    # Display predicted labels\n",
    "    axs[0, i+2].imshow(color_code_segment(label_class))\n",
    "    axs[0, i+2].set_title(f\" GT: {class_name}\", fontsize=title_font)\n",
    "    axs[0, i+2].axis('off')\n",
    "\n",
    "    # Display ground truth labels\n",
    "    axs[1, i+2].imshow(color_code_segment(pred_class))\n",
    "    axs[1, i+2].set_title(f\" Pred: {class_name}\", fontsize=title_font)\n",
    "    axs[1, i+2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.version.cuda) \n",
    "print(torch.cuda.is_available()) \n",
    "print(cv2.__version__)\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchvision version:\", torchvision.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
